# Quozen Agentic UI

## Context and Motivation:

We have a client-side serverless app PWA code and core package QuozenClient and my API (read the other MD files to understand architecture). We have most components needed to let our users use this app via an AI agent. 

If the user has a ChatGTP paid account, or Claude, we could just add an MCP server in cloudflare and direct them to add the connector to their agents. But we also want a way of letting users use free web based chatbots like Google Gemini with custom Gems, or just ChatGPT standard free.

Now, letÂ´s consider the following: what about also building a small chat app where all I have to ask the user to do is to provide me an API key (and if not I can use Googleâ€™s Cloud free api key with my user for a limited experience). Having the chance of getting an API and adding something to my PWA app could provide a better full chat experience.

## Agentic Client-side App

This approach is the **"Gold Standard" for modern AI-integrated PWAs**. By moving the "Chatbot" inside the application, we transition from a passive tool to an **Agentic UI**.

Since we already have `@quozen/core` running in the browser and an API (`apps/api`) deployable in Cloudflare Workers and Vercel Edge, we can build a **Hybrid Client-Side Agent**.

### **1\. The Architecture: "The Browser-Based Agent"**

In this model, the "Agent" is just a React Hook that orchestrates three things:

1. **The Brain (LLM):** Decides *what* to do (via API Key or Proxy).  
2. **The Context (RAG):** Reads current state from store using QuozenClient and React app state.  
3. **The Hands (Tools):** Executes functions directly using our existing `QuozenClient`.

#### **The Flow**

1. **User:** "Split lunch $50 with Alice."  
2. **App (Context):** Grabs current Group ID and Member List from `QuozenClient`.  
3. **App (Prompt):** Sends to LLM: *"User said 'Split lunch...'. Tools available: `addExpense`. Current Members: Alice (id: u1), Bob (id: u2)."*  
4. **LLM (Decision):** Returns JSON: `{ "tool": "addExpense", "args": { "amount": 50, "split": {"u1": 25, "u2": 25} } }`.  
5. **App (Execution):** Calls `quozen.ledger(groupId).addExpense(...)`.  
6. **UI:** Updates instantly.

### **2\. Implementation Strategy**

We have two distinct paths to handle the "API Key" dilemma. We should implement **both** for a tiered experience.

#### **Option A: "Bring Your Own Key" (Power Users)**

* **How:** User enters their personal OpenAI/Gemini/Anthropic key in `Settings`.  
* **Storage:** Encrypted using the API proxy, the encrypted value is obtained from the Edge function and is stored in userâ€™s quozen-settings.json.  
* **Pros:** Unlimited usage, privacy (user trusts their own key), access to smarter models (GPT-4o, Claude 3.5).  
* **Cons:** Friction for non-tech users.

#### **Option B: "The Quozen Free Tier" (Casual Users)**

* **How:** We use the `apps/api-proxy` (Cloudflare Worker) as a proxy.  
* **Security:**  
  * **Do NOT** put the Google Cloud Key in the client code.  
  * Store the key as a secret in Cloudflare (`wrangler secret put GOOGLE_API_KEY`) or Vercelâ€™s keys.  
  * Create a route `POST /api-proxy/v1/agent/chat`.  
  * The PWA sends the message history to this endpoint.  
  * The Worker calls Gemini Flash (very cheap/free tier) and returns the response.  
* **Rate Limiting:** Use Cloudflare's Rate Limiting or Vercelâ€™s own infrastructure, to prevent abuse of your quota.

### **3\. "Magic" Feature: Chrome Built-in AI (Gemini Nano)**

Since weâ€™re building a PWA, we can leverage **`window.ai`** (currently in Chrome Canary/Dev).

* **Concept:** The LLM runs *inside* the user's browser locally.  
* **Cost:** $0.  
* **Privacy:** 100% Local.  
* **Latency:** Near zero.  
* **Implementation:** Check if `window.ai` exists. If yes, use it. If not, ask for API Key. Future improvement: we can deploy the react app as a true local native mobile app and embed the nano LLM there.

### **4\. Technical Blueprint**

**Phase 1: Define the "Tools"** We need to map your `@quozen/core` methods to a JSON Schema that LLMs understand.

// src/lib/agent/tools.ts

export const QUOZEN\_TOOLS \= \[

  {

    name: "add\_expense",

    description: "Adds a new expense to the ledger.",

    parameters: {

      type: "object",

      properties: {

        description: { type: "string" },

        amount: { type: "number" },

        payerId: { type: "string", description: "The ID of the user who paid" },

        involvedMemberIds: { type: "array", items: { type: "string" } }

      },

      required: \["description", "amount", "payerId"\]

    }

  },

  {

    name: "get\_balance",

    description: "Checks who owes money.",

    parameters: { type: "object", properties: {} }

  }

\];

**Phase 2: The Agent Hook (`useAgent.ts`)** This hook connects the UI to the Logic.

// Simplified logic for apps/webapp/src/hooks/use-agent.ts

export function useAgent() {

  const { quozen } \= useAppContext();


  const sendMessage \= async (userText: string) \=\> {

    // 1\. Gather Context (RAG)

    const settings \= await quozen.groups.getSettings();

    const activeGroup \= settings.groupCache.find(g \=\> g.id \=== settings.activeGroupId);

    

    // 2\. Construct System Prompt

    const systemPrompt \= \`

      You are Quozen AI. 

      Current Group: ${activeGroup.name}

      Members: ${JSON.stringify(activeGroup.members)}

      Current Date: ${new Date().toISOString()}

    \`;

    // 3\. Call LLM (Pseudo-code for Strategy Router)

    const response \= await callLLM({

      provider: userHasKey ? 'DIRECT' : 'PROXY', 

      messages: \[{role: 'system', content: systemPrompt}, {role: 'user', content: userText}\],

      tools: QUOZEN\_TOOLS

    });

    // 4\. Handle Tool Calls

    if (response.tool\_calls) {

       for (const tool of response.tool\_calls) {

          if (tool.name \=== 'add\_expense') {

             await quozen.ledger(activeGroup.id).addExpense(tool.arguments);

             return "Expense added successfully\!";

          }

       }

    }

    

    return response.content; // Just chat

  };

  return { sendMessage };

}

# IMPLEMENTATION PLAN

### **Architectural Analysis**

Tightly coupling experimental AI features into the core DOM tree is a common trap that inflates bundle sizes and introduces breaking points if remote services degrade. Instead, we will use a **"Feature Module" pattern combined with React Lazy Loading and Context**.

Here is how we integrate it cleanly:

1. **The AI Provider (`AiFeatureProvider`):** We wrap the `AppLayout` in a new context. This provider evaluates the configuration (Auto, BYOK, Local, Cloud) and pings the Edge API/Browser to check availability. If everything fails or the user disables it, the provider sets `isAvailable: false`.  
2. **Lazy Loading (Code Splitting):** All AI logic (`useAgent`, the RAG builder, the `AICommandDrawer`) will live in a dedicated folder (e.g., `src/features/agent`). We will use `React.lazy()` to import it. If `isAvailable` is false, Vite will never even download the AI JavaScript bundle to the user's phone.  
3. **Component Slots (Inversion of Control):** The `Header` component will not import the `Sparkle` button directly. Instead, the `Header` will have a generic `<div id="header-actions-slot" />`. If the AI module loads successfully, it uses a React Portal to inject the Sparkle button into that slot.

This guarantees that if the AI services go down, the core Quozen app remains 100% untouched, fully functional, and lightweight.

### UX/UI analysis

Regarding UI/UX experience, here is how we can refine your UX proposal to perfectly match Quozen's recent mobile-first design system (which relies heavily on Bottom Drawers):

**1\. The Trigger: The "Sparkle" Icon**

* **Placement:** The upper right corner of the global `Header` is perfect. We can place a `Sparkles` (âœ¨) icon right next to the sync/refresh button.  
* **Visibility:** It will be visible on the Dashboard and Expenses tabs, giving users global access to command the app.

**2\. The Interface: The "AI Command Drawer"**

* Instead of a floating chat window, tapping the sparkle icon opens a **Bottom Drawer** (using your existing `Vaul` drawer implementation).  
* This drawer takes up about 50% of the screen.  
* It contains a large, auto-focused `<textarea>` that says: *"What do you want to do? (e.g., 'Split a $50 Uber with Bob')"*  
* **Voice Input:** We don't need to build a custom microphone/speech-to-text API. By simply using a standard HTML textarea, mobile users can tap the native microphone icon on their iOS/Android keyboard to dictate their commands with perfect OS-level accuracy.

**3\. The Interaction Loop (Feedback)**

1. User types/dictates: *"Alice paid 40 bucks for drinks, split with me."*  
2. User hits the **Submit** arrow.  
3. The Drawer's UI immediately switches to a **"Thinking..." state** (a subtle glowing animation or spinner).  
4. The React Hook (`useAgent`) performs the RAG context build and LLM routing (as we defined earlier).  
5. Upon successful execution, the **Drawer automatically slides down and closes**.  
6. A **Toast Message** appears: *"âœ… Added $40 for drinks (Paid by Alice)."*  
7. The underlying Dashboard UI updates instantly via React Query invalidation.

**4\. Transparency Badging**

* Inside this Command Drawer, just below the input box, we will place the tiny dynamic badge we discussed earlier (e.g., *"âš¡ Powered by Your Key"* or *"ðŸ’» On-Device AI"*). This reassures the user right where they input their data.

By combining a **Global Sparkle Trigger**, an **AI Command Bottom Drawer**, and **Toast-based Feedback**, we keep the app feeling incredibly fast, native, and uncluttered.

### Summary of Components to Implement

We now have all the pieces for this massive feature:

1. **The Agent Hook & RAG Context Builder**  
2. **The Edge KMS & Proxy** (for secure BYOK encryption and Team Key rate-limiting)  
3. **The 'Auto' Routing Waterfall** (BYOK \-\> Local AI \-\> Team Key)  
4. **The Command Drawer UX** (Invisible execution with Toast feedback)

### **Epic Definition**

#### **1\. THE EPIC**

**Title:** Modular Client-Side Agentic UI with Edge KMS 

**Description:** Introduce an Agentic UI allowing users to execute ledger commands via natural language (e.g., "Split a $50 Uber with Alice"). The feature acts as a strictly decoupled, lazy-loaded module. The LLM functions solely as a "Decision Engine" while the React App executes operations via the existing `@quozen/core` SDK. To optimize cost and privacy, the system automatically routes prompts through a waterfall: User's Encrypted Key (BYOK) âž” Free Browser AI (`window.ai`) âž” Rate-Limited Team Key (Vercel Edge Proxy). **Success Metrics:**

* **Modularity:** Core app bundle size increases by `< 5kb` when the AI module is not activated.  
* **Intent Accuracy:** \>95% of natural language inputs correctly map to the intended Quozen tool.  
* **Zero-Leakage:** 0 instances of Team API Keys or User API Keys leaking in network payloads or unencrypted Drive files.  
* **Quota Stability:** The Team API Key proxy strictly enforces daily limits per Google User ID via Vercel KV.

#### **2\. SCOPE & CONSTRAINTS (For the Architect)**

**In-Scope:**

* **Feature Flagging:** `AiFeatureProvider` and `React.lazy()` for strict code-splitting of the AI module.  
* **UX:** A "Sparkle" trigger button portaled into the Header, opening a Vault Bottom Drawer for natural language/dictation input, yielding Toast notifications on success.  
* **Edge KMS & Proxy:** Vercel Edge API routes (`/encrypt` and `/chat`) utilizing `window.crypto` for server-side encryption/decryption, and `@upstash/ratelimit` for quota management.  
* **RAG Context:** A lightweight prompt builder injecting Members, Balances, and Tool Schemas (excluding expense history).  
* **Auto-Routing Waterfall:** Logic to evaluate and fallback between BYOK, Local, and Cloud AI.  
* **New Workspace Module (`apps/ai-proxy`):** A strictly isolated Vercel Edge project dedicated to AI routing, KMS, and rate-limiting.  
* **Feature Flagging:** `AiFeatureProvider` and `React.lazy()` for strict code-splitting of the AI module in the React frontend. The React app must continue operating regardless of availability of AI or not. The feature may also be included or excluded via build environment parameters.  
* **Edge KMS & Proxy (Isolated):** Edge API routes (`/encrypt` and `/chat`) within the `ai-proxy` module utilizing Web Crypto for server-side encryption/decryption, and `@upstash/ratelimit` for quota management.  
* **Auto-Routing Waterfall:** Client-side logic to evaluate and fallback between BYOK, Local Browser AI, and the `ai-proxy` Cloud endpoint.

**Out-of-Scope:**

* Autonomous background agents (execution is strictly synchronous to user prompts).  
* Entire "Agent Views" or generative UI (the UI remains standard Quozen; the AI is just a command interface).  
* Changing the base React Router configuration.  
* **Modifying `apps/api`:** The existing core REST API must remain completely untouched by AI dependencies or KMS logic.  
* Autonomous background agents (execution is strictly synchronous to user prompts).

**Technical Dependencies:**

* `window.crypto.subtle` (Web Crypto API).  
* Chrome `window.ai` API.  
* `@upstash/ratelimit` and `@vercel/kv` (Restricted entirely to `apps/ai-proxy`).  
* OpenAI/Anthropic SDKs (Restricted entirely to `apps/ai-proxy`).

**NFRs:**

* **Fault Tolerance:** If the AI module crashes, the network is down, or `window.ai` hallucinates, the core Quozen ledger experience must remain 100% functional.  
* **Security:** The Edge function must validate the Google OAuth Bearer token before performing any KMS or Proxy operations.  
* **Separation of Concerns:** The `ai-proxy` must not directly interact with Google Drive. It acts purely as a stateless text-in/JSON-out intelligence and encryption layer. The `webapp` remains responsible for executing the returned JSON against Google Drive.

---

#### **3\. USER STORIES (For the Engineers)**

* **US-101: Pluggable Infrastructure & Auto-Routing Waterfall**  
  * **Narrative:** As an Architect, I want the AI feature to be completely isolated and code-split, So that it does not bloat the core app or cause fatal crashes if AI services are down.  
  * **Scenario 1 (Module Loading):** Given the app loads, When `AiFeatureProvider` evaluates the environment, Then it dynamically imports the `AgentModule` using `React.lazy()` only if the user hasn't explicitly disabled AI.  
  * **Scenario 2 (Auto-Routing):** Given the AI is active, When initializing, Then it checks state in this exact order: 1\) Is an encrypted BYOK present in `quozen-settings.json`? \-\> Use Edge KMS. 2\) Does `window.ai.canCreateTextSession()` return 'readily'? \-\> Use Local AI. 3\) Else \-\> Use Edge Proxy (Team Key).  
  * **Dev Notes:** Add `preferences.aiProvider: 'auto' | 'byok' | 'local' | 'cloud'` to `UserSettings`.  
      
* **US-102: Isolated AI Proxy & Edge KMS Microservice**  
* **Narrative:** As a System Architect, I want the AI capabilities and KMS logic isolated in a separate monorepo package (`apps/ai-proxy`), So that the core REST API, The core QuozenClient and the React app remains lightweight, secure, and free of third-party LLM dependencies.  
* **Scenario 1 (Infrastructure Setup):** Given the monorepo structure, When the developer builds the AI features, Then they initialize a new Hono Edge app at `apps/ai-proxy` with its own `package.json`, `vercel.json`, and isolated environment variables (`LLM_API_KEY`, `KMS_SECRET`, `KV_REST_API_URL`).  
* **Scenario 2 (Encrypting BYOK):** Given the user enters an API key, When the client POSTs to the new proxy's `/encrypt` endpoint, Then the isolated Edge function encrypts it and returns the ciphertext to the client.  
* **Scenario 3 (Rate Limiting & Chat):** Given the user utilizes the Team Key, When they POST to the proxy's `/chat` endpoint, Then Vercel KV checks their Google User ID. If under the limit, it calls the LLM and returns the structured JSON.  
* **Dev Notes:** Ensure the `apps/ai-proxy` endpoints are still protected by the same Google Auth middleware logic used in `apps/api` so that only authenticated Quozen users can consume the AI bandwidth. Since this is decentralized and anyone with a Google account can use it, we may need a way to make requests accepted only if coming from authorized URL domains, where the react app is deployed.  
* **US-103: The AI Command Drawer UX**  
  * **Narrative:** As a User, I want a fast, native-feeling drawer to speak or type my commands, So that I don't feel like I'm leaving the app to use a chat window.  
  * **Scenario 1 (Trigger & UI):** Given the AI module is loaded, When I am on the Dashboard, Then a "Sparkle" icon appears in the Header. Tapping it opens a 50% height bottom drawer with an auto-focused `<textarea>`.  
  * **Scenario 2 (Transparency Badge):** When the drawer opens, Then a small dynamic badge sits below the input indicating the active engine (e.g., "âš¡ Powered by Your Key" or "ðŸ’» On-Device AI").  
  * **Scenario 3 (Feedback Loop):** Given I submit a command, When the orchestrator executes it successfully, Then the Drawer automatically closes and a success Toast (e.g., "Added $50 for lunch") appears.  
* **US-104: The Agent Orchestrator & RAG Builder**  
  * **Narrative:** As the React Client, I need to build a context-rich prompt and safely parse the output, So that the LLM makes correct decisions based on the current group state.  
  * **Scenario 1 (Context Construction):** When a user submits "Split lunch $50 with Bob", Then the hook fetches the active `Ledger`, extracts the `Members` array (IDs and Names) and `Balances`, and appends the JSON schemas for `addExpense` and `addSettlement`.  
  * **Scenario 2 (Execution):** When the LLM returns the JSON intent, Then the hook maps it to the `QuozenClient`, executes the mutation, and invalidates the `['drive', 'group', id]` React Query to instantly refresh the UI.  
* **US-105: Browser AI (`window.ai`) Fallback**  
  * **Narrative:** As a Chrome user, I want to use free, local AI, So that I don't consume network quotas.  
  * **Scenario 1 (Few-Shot Prompting):** Given `window.ai` is the active router, When the RAG prompt is built, Then it is aggressively prepended with at least 3 examples of User Input \-\> Expected JSON Output to prevent hallucinations.  
  * **Scenario 2 (JSON Extraction):** Given `window.ai` wraps the response in Markdown (e.g., ```` ```json {...} ``` ````), When the client receives it, Then it strips the formatting via regex before parsing.

